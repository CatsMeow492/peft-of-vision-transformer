⸻

📄 Core LoRA & PEFT Papers
	•	LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)
https://arxiv.org/abs/2106.09685
	•	AdaLoRA: Adaptive Budget Allocation for Low-Rank Adaptation (Zhang et al., 2023)
https://openreview.net/forum?id=3Qj3xSwN2I

⸻

🌐 BitsAndBytes / Quantized LoRA
	•	“A Gentle Introduction to 8-bit Matrix Multiplication for Transformers” (Hugging Face + bitsandbytes)
https://huggingface.co/blog/hf-bitsandbytes-integration
	•	BitsAndBytes library documentation (quantization tools for Transformers)
https://huggingface.co/docs/transformers/main/quantization/bitsandbytes
	•	bitsandbytes GitHub (k-bit quantization for PyTorch)
https://github.com/bitsandbytes-foundation/bitsandbytes

⸻

🖼️ Vision-Adapter Research
	•	Parameter-Efficient Fine-Tuning for Vision Transformers (RLRR, CvPR 2023)
(No direct URL publicly; adapt via search)
	•	Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer (Li et al., 2022)
https://arxiv.org/abs/2210.06707

⸻

🧪 Quantization & QLoRA Variants
	•	Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations (Hubara et al., 2016)
https://arxiv.org/abs/1609.07061

⸻

🔧 Tools & Tutorials
	•	Medium article: Model Quantization with HuggingFace + bitsandbytes
https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996

⸻
