â¸»

ğŸ“„ Core LoRA & PEFT Papers
	â€¢	LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)
https://arxiv.org/abs/2106.09685
	â€¢	AdaLoRA: Adaptive Budget Allocation for Low-Rank Adaptation (Zhang et al., 2023)
https://openreview.net/forum?id=3Qj3xSwN2I

â¸»

ğŸŒ BitsAndBytes / Quantized LoRA
	â€¢	â€œA Gentle Introduction to 8-bit Matrix Multiplication for Transformersâ€ (Hugging Face + bitsandbytes)
https://huggingface.co/blog/hf-bitsandbytes-integration
	â€¢	BitsAndBytes library documentation (quantization tools for Transformers)
https://huggingface.co/docs/transformers/main/quantization/bitsandbytes
	â€¢	bitsandbytes GitHub (k-bit quantization for PyTorch)
https://github.com/bitsandbytes-foundation/bitsandbytes

â¸»

ğŸ–¼ï¸ Vision-Adapter Research
	â€¢	Parameter-Efficient Fine-Tuning for Vision Transformers (RLRR, CvPR 2023)
(No direct URL publicly; adapt via search)
	â€¢	Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer (Li et al., 2022)
https://arxiv.org/abs/2210.06707

â¸»

ğŸ§ª Quantization & QLoRA Variants
	â€¢	Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations (Hubara et al., 2016)
https://arxiv.org/abs/1609.07061

â¸»

ğŸ”§ Tools & Tutorials
	â€¢	Medium article: Model Quantization with HuggingFace + bitsandbytes
https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996

â¸»
