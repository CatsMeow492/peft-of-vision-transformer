{
  "name": "quant_deit_tiny_patch16_224_cifar10_r4_s123",
  "description": "Quantization experiment: deit_tiny_patch16_224 on cifar10 with 4-rank LoRA",
  "tags": [
    "quantization",
    "peft",
    "lora",
    "task_8_2"
  ],
  "model": {
    "name": "deit_tiny_patch16_224",
    "source": "timm",
    "pretrained": true,
    "num_classes": 10,
    "image_size": 224,
    "patch_size": 16
  },
  "dataset": {
    "name": "cifar10",
    "data_dir": null,
    "train_split": "train",
    "val_split": "validation",
    "test_split": "test",
    "image_size": 224,
    "normalize": true,
    "augmentation": true,
    "batch_size": 32,
    "num_workers": 4,
    "pin_memory": true
  },
  "lora": {
    "rank": 4,
    "alpha": 8.0,
    "dropout": 0.1
  },
  "quantization": null,
  "training": {
    "learning_rate": 0.0001,
    "batch_size": 32,
    "num_epochs": 15,
    "warmup_steps": 100,
    "weight_decay": 0.01,
    "optimizer": "adamw",
    "scheduler": "cosine",
    "gradient_clip_norm": 1.0,
    "use_mixed_precision": true,
    "gradient_accumulation_steps": 1,
    "save_steps": 500,
    "eval_steps": 100,
    "logging_steps": 25,
    "early_stopping_patience": 5,
    "output_dir": "outputs",
    "logging_dir": null,
    "seed": 42
  },
  "seed": 123,
  "output_dir": "experiments/outputs",
  "use_adalora": false,
  "use_qa_lora": false,
  "max_memory_gb": 24.0,
  "max_training_time_hours": null
}